---
title: "Research Design"
module_number: 3
author: "Vivek H. Patil, Ph.D."
institution: "Gonzaga University"
subtitle: "Choosing among exploratory, descriptive, and causal designsâ€”and understanding threats to validity."
learning_objectives:
  - "Explain how the degree of uncertainty determines the choice of research design"
  - "Describe the purposes and characteristics of exploratory research"
  - "Describe the purposes and characteristics of descriptive research, including cross-sectional and longitudinal designs"
  - "Describe the purposes and characteristics of causal research and its requirements"
  - "Identify and explain threats to internal validity"
  - "Distinguish between internal and external validity"
  - "Compare laboratory and field experiments"
  - "Match research designs to appropriate data collection methods"
discussion_questions:
  - "A mid-size retail company notices that its online customer reviews have become increasingly negative over the past six months. The VP of Marketing wants to 'figure out what's going on.' What type of research design would you recommend as a first step? Why? What design might follow?"
  - "Think of a business experiment you have encountered (e.g., A/B testing on a website, a promotional test in select stores). Identify at least two threats to internal validity that might affect the results. How could these threats be mitigated?"
  - "The distinction between laboratory and field experiments involves a trade-off between internal and external validity. Under what business circumstances would you prioritize internal validity? When would external validity be more important?"
  - "Consider how the Jesuit commitment to truth-seeking applies to causal research. What are the ethical implications of designing experiments that intentionally expose some participants to different conditions (e.g., different prices, different service levels)?"
---

## 3.1 Linking Problems to Designs

In Module 2, we learned how to formulate research problems, objectives, and hypotheses. The next critical decision is: **what type of research design will best address the question?**

The choice of research design is fundamentally determined by **the degree of uncertainty** surrounding the problem. When the problem is poorly understood and the researcher needs to clarify what is happening, an exploratory design is the appropriate starting point. Exploratory research provides the flexibility needed to investigate ill-defined situations, uncover relevant variables, and develop preliminary understanding. When the problem is well-defined and the researcher needs to describe the characteristics of a population or phenomenon with precision, a descriptive design is appropriate. Descriptive research relies on structured methods to produce quantifiable results that characterize the "who, what, when, where, and how" of a situation. When the central question concerns whether one variable causes a change in another, a causal design is required. Causal research employs experimental methods to test hypotheses about cause-and-effect relationships with the rigor necessary to support confident conclusions.

These three designs are not mutually exclusive. As the surgical mask case study in Module 2 illustrated, a research program may progress through all three stages: exploratory research to identify possible causes, descriptive research to quantify their prevalence, and causal research to test whether a proposed solution works. This progression from ambiguity to certainty is one of the most common patterns in applied business research. A company might begin with qualitative interviews to understand why customer satisfaction is declining (exploratory), follow up with a large-scale survey to measure the extent of the problem across different segments (descriptive), and finally run a controlled experiment to test whether a new service protocol improves satisfaction scores (causal). Each stage builds on the insights of the previous one, and the degree of structure increases as uncertainty decreases.

<div class="callout callout-key-concept">
<p class="callout-title">Key Concept</p>
<p>The degree of uncertainty about the research problem determines the methodology. Greater uncertainty calls for more flexible, exploratory approaches. Greater certainty enables more structured, confirmatory designs.</p>
</div>

## 3.2 Exploratory Research

**Exploratory research** is conducted when the researcher has limited knowledge about the problem. It is the research equivalent of reconnaissance: before committing to a full-scale investigation, the researcher surveys the terrain, identifies the key features of the landscape, and determines where to direct more focused efforts. The purposes of exploratory research are varied and far-reaching (Zikmund et al., 2013).

At its most fundamental level, exploratory research serves to formulate a problem for more precise investigation or to develop hypotheses that can be tested in subsequent studies. A marketing manager who senses that "something is off" with customer engagement, for example, might commission exploratory research to move from that vague intuition to a concrete, testable proposition about what is driving the change. By conducting interviews, reviewing secondary data, or facilitating group discussions, the researcher helps transform an ambiguous sense of unease into a well-defined research problem.

Exploratory research also plays a critical role in establishing priorities for further investigation. In most business settings, research budgets and timelines are limited. A company facing multiple possible explanations for declining sales cannot afford to investigate all of them with equal intensity. Exploratory work helps identify which explanations are most plausible and which questions are most urgent, allowing the organization to allocate its research resources wisely. Closely related to this prioritization function is the practical benefit of gathering information about the challenges of conducting research on a particular topic. Before launching an expensive survey or a complex experiment, exploratory research can reveal logistical obstacles, sensitive issues that may affect respondent cooperation, or definitional ambiguities that would undermine data quality if not addressed in advance.

Beyond these planning and prioritization functions, exploratory research serves several substantive purposes. It increases the researcher's familiarity with the problem domain, which is especially important when entering unfamiliar territory such as a new market, a new product category, or a new cultural context. It helps clarify concepts that may be used loosely in everyday business language but require precise definition for rigorous research. For instance, what exactly does "customer loyalty" mean in a particular industry context? Does it refer to repeat purchasing, willingness to recommend, emotional attachment, or some combination of these? Exploratory research can also generate new product or service concepts, identify problem solutions, or compile lists of product features that consumers value. It provides a mechanism for getting preliminary reactions to new ideas before investing heavily in their development. In the domain of survey research, exploratory work is invaluable for pretesting structured questionnaires, ensuring that questions are understood as intended and that response options are comprehensive. More broadly, exploratory research helps researchers understand the consumer's perspective, vocabulary, needs, and usage situations, laying the groundwork for research instruments that speak the respondent's language rather than the researcher's.

### Characteristics of Exploratory Research

Several defining characteristics distinguish exploratory research from other design types. First, exploratory research is flexible and unstructured. Unlike descriptive or causal designs, which follow predetermined protocols, exploratory studies allow the researcher to adapt the approach as new information emerges. An interviewer may pursue an unexpected line of questioning; a researcher reviewing secondary data may shift focus when a surprising pattern appears. This adaptability is not a weakness but a strength: it enables the researcher to follow the evidence wherever it leads, rather than being constrained by preconceptions about what the findings should look like.

Second, exploratory research is predominantly qualitative in nature. The typical methods include focus groups, depth interviews, case studies, and unstructured observation. These methods are designed to generate rich, detailed, contextual understanding rather than numerical summaries. They excel at uncovering the "why" behind behavior, the meanings that people attach to their experiences, and the language they use to describe their world.

Third, exploratory studies tend to be small-scale. Sample sizes are usually limited and are not intended to be statistically representative of a larger population. The goal is depth rather than breadth, insight rather than generalization. A series of ten in-depth interviews with dissatisfied customers may reveal more about the nature of their dissatisfaction than a survey of a thousand customers with closed-ended questions.

Fourth, and perhaps most importantly, exploratory research is hypothesis-generating rather than hypothesis-testing. The researcher enters the study with questions, not answers. The objective is to emerge with plausible explanations, tentative frameworks, and testable propositions that can then be subjected to more rigorous investigation through descriptive or causal research.

### When to Use Exploratory Research

Exploratory research is most appropriate under several conditions. It is the design of choice when the research problem is ambiguous or vague, when management knows that something is wrong or that an opportunity exists but cannot articulate the problem with enough precision to guide structured research. It is also the right approach when the researcher is entering an unfamiliar domain, such as a new geographic market, a new product category, or a new cultural context where assumptions about consumer behavior may not hold. When the existing literature provides little guidance on a topic, perhaps because it is genuinely novel or because prior research has approached it from a different angle, exploratory research helps build the foundational understanding that more structured designs require. Finally, exploratory research is essential when the researcher needs to understand how people think about, talk about, or experience a phenomenon. If the goal is to capture the richness and complexity of human experience rather than to reduce it to numerical summaries, exploratory methods are indispensable.

## 3.3 Descriptive Research

**Descriptive research** aims to describe the characteristics of a population, phenomenon, or market situation. It answers questions of **who, what, when, where, and how** (though not necessarily *why*). Where exploratory research seeks to illuminate the unknown, descriptive research seeks to measure the known with precision and accuracy.

Descriptive studies serve a range of important functions in business research. They can describe the characteristics of groups, such as the demographic profile, media habits, or purchasing patterns of a brand's customers. They can estimate the proportion of people who behave in a certain way, for example, the percentage of consumers who shop online at least once a week, or the share of small businesses that use social media for marketing. They can also make specific predictions about the relationship between marketing variables, such as predicting how changes in advertising spending relate to changes in brand awareness, or how customer satisfaction scores vary across different service channels. In each case, the emphasis is on accurately measuring and describing what exists, rather than on understanding why it exists or on manipulating variables to produce a desired outcome.

### Characteristics of Descriptive Research

Unlike exploratory research, descriptive studies are structured and rigid in their design. The research questions, data collection instruments, and sampling procedures are determined in advance, and the researcher follows these protocols consistently throughout the study. This rigidity is not a limitation but a necessity: standardized procedures are what enable the researcher to make valid comparisons across respondents, groups, or time periods and to draw statistically defensible conclusions from the data.

Descriptive research is predominantly quantitative in its orientation. Surveys and structured observations are the most typical methods, producing numerical data that can be analyzed using statistical techniques. Sample sizes tend to be larger than in exploratory studies because statistical representativeness is a central goal. The researcher wants to be able to say, with a known degree of confidence, that the results obtained from the sample reflect the characteristics of the broader population from which the sample was drawn.

In some cases, descriptive research can also serve a hypothesis-testing function. While descriptive designs cannot establish causation, they can test hypotheses about the existence and strength of relationships between variables. A researcher might hypothesize that customer satisfaction is positively related to repurchase intention and use survey data to test that hypothesis. The finding that such a relationship exists does not prove that satisfaction causes repurchase, but it does provide evidence that the two variables covary in the predicted direction, which is a necessary (though not sufficient) condition for causation.

### An Accurate Snapshot

Descriptive research provides an accurate snapshot of some aspect of the market environment. The power of descriptive research lies in its ability to convert vague impressions into precise, quantifiable portraits of reality. Consider the difference between a manager's sense that "most of our customers seem to support our sustainability initiative" and a descriptive study finding that "73 percent of our active customers, plus or minus 3 percentage points at the 95 percent confidence level, report that the company's sustainability efforts positively influence their purchase decisions." The latter statement provides a basis for decision-making that the former cannot.

Typical examples of descriptive research outputs include estimates of the proportion of the adult population that supports a given charitable cause, comparative evaluations of a product's attributes relative to competing products, detailed profiles of the socioeconomic and demographic characteristics of a magazine's readership, and assessments of the proportion of retail outlets that carry, display, or actively merchandise a particular product. In each case, the descriptive study transforms a question about the state of the world into a precise, evidence-based answer.

### Types of Descriptive Studies

Descriptive studies can be classified as:

| Type | Description | Example |
|---|---|---|
| **Cross-sectional** (Sample Survey) | Data collected from a sample at a single point in time | A customer satisfaction survey conducted in March |
| **Longitudinal** | Data collected from the same (or comparable) sample over time | A panel study tracking the same consumers' purchasing behavior monthly |

Longitudinal designs can be further divided into two subtypes. **True panels** survey the same individuals repeatedly on the same topics, allowing researchers to track changes at the individual level and to identify which specific people changed their behavior or attitudes over time. **Omnibus panels** also survey the same individuals repeatedly, but the topics may change from one wave to the next, providing flexibility to address emerging questions while retaining the advantage of a stable, pre-recruited sample.

Longitudinal designs are powerful for studying change over time but are more expensive and subject to **panel attrition** (participants dropping out over time). Attrition is not merely a logistical inconvenience; it can introduce systematic bias if the people who drop out differ in meaningful ways from those who remain. For example, if less satisfied customers are more likely to leave a panel, the remaining sample will overrepresent satisfied customers, producing an overly optimistic picture of customer sentiment. Researchers must monitor attrition carefully and, where possible, compare the characteristics of those who remain with those who leave.

## 3.4 Causal Research

**Causal research** seeks to identify cause-and-effect relationships. When a manager asks, "Will changing X lead to a change in Y?"---that is a causal question. Causal research typically involves **experiments**.

### Requirements for Establishing Causation

To claim that variable X causes variable Y, three conditions must be satisfied (adapted from Churchill & Iacobucci, 2015):

**1. Concomitant Variation**
X and Y must occur together (or vary together) in the way predicted by the hypothesis. If we hypothesize that advertising spending increases brand awareness, we should observe that higher ad spending is associated with higher awareness. However, correlation alone is not sufficient for causation.

*Examples*:
- Sales person compensation --> Sales performance
- Ad severity (fear appeal) --> Ad effectiveness

**2. Temporal Antecedence (Time Order)**
The cause (X) must occur **before** the effect (Y). If we claim that a training program improves performance, we must show that performance improved *after* the training was implemented, not before.

**3. Elimination of Other Possible Causal Factors (Nonspurious Association)**
We must have evidence that allows us to rule out alternative explanations. If both advertising spending and brand awareness increased, but so did a competitor's product recall, the awareness increase might be due to the recall rather than our advertising. This is the requirement of **internal validity**---the degree to which the observed effect can be attributed to the experimental treatment rather than to confounding factors.

> "When you have eliminated the impossible, whatever remains, however improbable, must be the truth."
> --- Arthur Conan Doyle, *The Sign of the Four* (1890)

While Sherlock Holmes may overstate the ease of elimination, the principle is sound: establishing causation requires systematically ruling out alternative explanations.

## 3.5 Threats to Internal Validity

**Internal validity** refers to the degree to which the observed results of an experiment can be attributed to the experimental treatment (the independent variable) rather than to other, extraneous factors. It is, in many ways, the most fundamental concern in causal research. If a study lacks internal validity, its conclusions about cause and effect are unreliable, regardless of how large the sample or how sophisticated the statistical analysis. Over decades of methodological scholarship, researchers have identified several well-documented threats that can undermine internal validity (Campbell & Stanley, 1963; Cook & Campbell, 1979). Each of these threats represents a specific way in which something other than the intended treatment could produce the observed results, creating an alternative explanation that the researcher must anticipate and address. Understanding these threats is not merely an academic exercise; it is essential for designing studies that yield credible conclusions and for critically evaluating research conducted by others. The following table summarizes the most commonly recognized threats.

| Threat | Description | Example |
|---|---|---|
| **History** | An external event (other than the treatment) occurs during the experiment and affects the outcome | A competitor launches a major promotion during your pricing experiment |
| **Maturation** | Natural changes in participants over time that are unrelated to the treatment | Employees become more experienced over the duration of a training study |
| **Testing** | The initial measurement itself affects subsequent responses | Taking a pretest on brand knowledge makes participants more attentive to brand information |
| **Instrumentation** | Changes in the measurement instrument, observers, or procedures over time | A survey question is reworded midway through data collection |
| **Selection** | Pre-existing differences between experimental and control groups | Volunteers for a new work program may already be more motivated than non-volunteers |
| **Mortality (Attrition)** | Participants drop out of the study before completion, and drop-outs differ systematically from those who remain | Dissatisfied customers are more likely to drop out of a longitudinal satisfaction study |

Researchers have developed a range of strategies to mitigate these threats, and the choice of mitigation strategy depends on the specific threat and the research context. Random assignment of participants to experimental and control groups is the single most powerful tool for addressing selection bias, because it ensures that any pre-existing differences between groups are distributed randomly rather than systematically. Control groups serve as a baseline against which the treatment effect can be measured, helping to account for the effects of history, maturation, and testing. When a pretest is necessary but testing effects are a concern, researchers may use a Solomon four-group design, which includes groups that receive the pretest and groups that do not, allowing the researcher to estimate and adjust for the effect of the pretest itself. Standardization of procedures and instruments throughout the study guards against instrumentation threats, while keeping the study duration as short as practical can reduce the impact of both maturation and history. To combat attrition, researchers may offer incentives for continued participation, maintain regular contact with participants, and conduct statistical analyses comparing the characteristics of those who complete the study with those who drop out. No single strategy eliminates all threats, but a well-designed experiment anticipates the most likely threats and incorporates appropriate safeguards.

## 3.6 External Validity

While internal validity asks "Did X really cause Y in this study?", **external validity** asks "Can we generalize these findings to other settings, populations, and times?" A study can have strong internal validity but limited external validity---and vice versa.

This tension is most apparent in the comparison between laboratory and field experiments. A tightly controlled laboratory study may provide compelling evidence that a particular pricing frame increases willingness to pay among college students viewing hypothetical products on a screen. But can we confidently generalize that finding to real consumers making real purchases in actual retail environments? External validity concerns whether the conditions of the study, the characteristics of the participants, and the nature of the task are sufficiently representative of the real-world situations to which the researcher wishes to apply the results. Studies conducted with narrow, homogeneous samples (such as university students), using artificial stimuli or tasks, in environments far removed from the actual decision-making context, face legitimate questions about the generalizability of their findings. Conversely, studies conducted in messy, real-world environments may produce highly generalizable results but may struggle to isolate the precise causal mechanism at work.

## 3.7 Laboratory vs. Field Experiments

| Dimension | Laboratory Experiment | Field Experiment |
|---|---|---|
| **Setting** | Artificial, controlled environment | Natural, real-world environment |
| **Realism** | Low (participants know they are in a study) | High (participants may be unaware) |
| **Control over extraneous variables** | High | Low |
| **Internal validity** | Typically higher | Typically lower |
| **External validity** | Typically lower | Typically higher |
| **Cost** | Generally lower | Generally higher |
| **Duration** | Usually shorter | Usually longer |
| **Participant awareness** | Participants are typically aware | Participants may be unaware |

### When to Choose Each

The choice between laboratory and field experiments is not a matter of one being inherently superior to the other; it is a strategic decision that depends on the research question, the stage of knowledge development, and the practical constraints facing the researcher.

Laboratory experiments are the design of choice when the researcher needs tight control over extraneous variables and wants to isolate the precise effect of one variable on another with maximum confidence. They are particularly well-suited to research questions about psychological mechanisms, such as how the framing of a message affects consumer choice, how cognitive load influences decision quality, or how social cues alter willingness to pay. In these cases, the artificial setting is not a weakness but a feature: by stripping away the noise of the real world, the laboratory allows the researcher to observe the phenomenon of interest in its purest form. Laboratory experiments also tend to be less expensive and faster to execute, making them an efficient tool for testing theoretical propositions before committing to larger-scale field studies. Doctoral students and academic researchers often rely on laboratory experiments during the early stages of a research program, when the priority is establishing that an effect exists under ideal conditions before investigating whether it holds in the field.

Field experiments, by contrast, are the preferred design when realism and generalizability are paramount. If the research question concerns how a marketing intervention will perform in an actual market setting, a field experiment provides evidence that a laboratory study simply cannot. Testing a new pricing strategy in real stores with real customers making real purchase decisions produces findings that managers can act upon with greater confidence, because the conditions of the study closely mirror the conditions under which the results will be applied. Field experiments are also the natural choice when the researcher wants to observe natural behavior rather than behavior in a contrived setting. People may behave differently when they know they are being studied (an effect known as demand characteristics or the Hawthorne effect), and field experiments, in which participants may be entirely unaware that they are part of a study, avoid this source of bias. The tradeoff, however, is that field experiments are typically more expensive, take longer to execute, and offer less control over extraneous variables, meaning that the researcher must accept greater uncertainty about whether the observed effect is truly caused by the treatment or by some unmeasured confound.

In practice, the strongest research programs often employ both approaches in sequence. A laboratory experiment may first establish that an effect exists and identify the conditions under which it is strongest. A field experiment can then test whether the effect holds in a realistic setting, providing the external validity needed to justify managerial action. This complementary use of laboratory and field methods reflects a mature understanding that no single study, however well-designed, can simultaneously maximize both internal and external validity.

<div class="callout callout-example">
<p class="callout-title">Example: A/B Testing as Field Experimentation</p>
<p>Modern digital marketing has made field experimentation accessible and affordable. <strong>A/B testing</strong>---randomly assigning website visitors to different versions of a page---is a form of field experiment. It takes place in a natural setting (the user's normal browsing), uses random assignment (providing internal validity), and measures real behavior (providing external validity). The rise of A/B testing illustrates how technology has shifted the balance, making it possible to achieve both internal and external validity simultaneously in certain contexts.</p>
</div>

## 3.8 Matching Designs to Data Collection Methods

Different research designs naturally pair with different data collection methods. The following table summarizes the most appropriate pairings (adapted from Churchill & Iacobucci, 2015):

| Data Collection Method | Exploratory | Descriptive | Causal |
|---|---|---|---|
| **Secondary data analysis** | Most appropriate | | |
| **Qualitative research** (interviews, focus groups, unstructured observation) | Most appropriate | | |
| **Surveys** (and structured observation) | | Most appropriate | |
| **Experiments** | | | Most appropriate |

These pairings are guidelines, not rules. Surveys can include exploratory questions, and experiments can include descriptive measures. But understanding the typical alignment helps researchers make sound design choices. The logic behind these pairings is straightforward. Secondary data analysis and qualitative methods are flexible, relatively unstructured, and well-suited to the open-ended nature of exploratory inquiry. Surveys impose structure on the data collection process, standardizing questions and response options in a way that supports the quantification and comparison goals of descriptive research. Experiments go further by manipulating independent variables and controlling extraneous ones, creating the conditions necessary for causal inference. As a researcher moves from exploration to description to causal testing, the methods become progressively more structured, more resource-intensive, and more capable of supporting definitive conclusions.

## 3.9 Qualitative vs. Quantitative: A Comparative Overview

A recurring theme across research designs is the distinction between qualitative and quantitative approaches. This distinction runs deeper than a simple difference in data type; it reflects fundamentally different assumptions about the nature of knowledge, the role of the researcher, and the purpose of inquiry. Qualitative research operates from the premise that human experience is complex, contextual, and best understood through immersion in the perspectives of those being studied. Quantitative research operates from the premise that patterns in human behavior can be identified, measured, and generalized through systematic observation and statistical analysis. Neither premise is wrong; each illuminates different aspects of reality.

The following comparison (adapted from McDaniel & Gates, 2002, Table 5.1) provides a useful summary of how these two approaches differ across several key dimensions:

| Dimension | Qualitative Research | Quantitative Research |
|---|---|---|
| **Nature of questions** | Open-ended, deep probing | Structured, limited probing |
| **Sample size** | Small | Large |
| **Information quality** | Rich, contextual, detailed | Varies; standardized |
| **Analytical skills** | Analysis of textual/non-numerical data | Analysis of numerical data |
| **Nature of analysis** | Subjective; higher potential for researcher bias | Statistical; lower potential for researcher bias |
| **Reliability (replicability)** | Lower | Higher |
| **Typical research type** | Exploratory | Descriptive or causal |

In practical terms, these differences translate into distinct strengths and limitations. Qualitative research excels at uncovering unexpected insights, capturing the richness of lived experience, and generating hypotheses about phenomena that are poorly understood. A series of in-depth interviews with customers who have recently defected to a competitor may reveal motivations that no survey could have anticipated, precisely because the researcher did not need to know in advance which questions to ask. Quantitative research, by contrast, excels at testing hypotheses, estimating the prevalence of known phenomena, and producing results that can be generalized to larger populations with statistical confidence. A well-designed survey can tell you not only that price sensitivity varies across customer segments but precisely how much it varies and which segments are most and least sensitive.

Neither approach is inherently superior. The choice depends on the research question, the stage of the research process, and the type of evidence needed. Many of the strongest research programs use **mixed methods**---qualitative research to understand phenomena and generate hypotheses, followed by quantitative research to test those hypotheses and quantify effects. A mixed-methods approach recognizes that understanding and measurement are complementary goals, and that the deepest insights often emerge at the intersection of the two traditions.

<div class="callout callout-jesuit">
<p class="callout-title">Jesuit Perspective</p>
<p>The Jesuit intellectual tradition values both depth and rigor---understanding the particular (qualitative) and the general (quantitative). <em>Discernment</em>, a cornerstone of Ignatian spirituality, involves careful reflection on evidence from multiple sources before making a decision. In research design, discernment manifests as the thoughtful selection of methods that are genuinely appropriate for the question at hand, rather than defaulting to familiar techniques. The best researchers, like the best leaders, match their tools to the task.</p>
</div>

## 3.10 Choosing the Right Design: A Decision Framework

Selecting the appropriate research design is one of the most consequential decisions a researcher makes, because the design determines what kinds of conclusions the study can support. A poorly chosen design can waste resources, produce misleading findings, or fail to answer the question that motivated the research in the first place. While there is no mechanical formula that produces the "right" design in every situation, a series of diagnostic questions can guide the researcher toward an informed choice.

The first question to ask is how well-defined the problem is. If the problem is vague, ambiguous, or poorly understood, the researcher should begin with an exploratory design. There is little point in conducting a large-scale survey or a carefully controlled experiment if the researcher is not yet sure what questions to ask or what variables matter. Exploratory research provides the understanding needed to formulate precise research questions and testable hypotheses. If the problem is already well-defined, the researcher can move directly to a descriptive or causal design, depending on the type of evidence required.

The second question concerns the type of evidence needed to address the research objective. If the goal is to develop understanding and context---to learn how consumers think about a product category, to identify the factors that influence a decision, or to map the range of experiences people have with a service---then qualitative, exploratory methods are appropriate. If the goal is to quantify the prevalence of a behavior, to estimate proportions or averages, or to document correlations between variables, then a descriptive design employing survey methods is the natural choice. If the goal is to establish cause and effect---to determine whether a specific intervention produces a specific outcome---then only a causal, experimental design will suffice.

The third consideration is the stage of the research program. Research rarely proceeds in a single leap from question to answer. More often, it unfolds in stages, with each stage building on the insights of the one before. In the early stages, when the researcher is generating ideas and building familiarity with the problem domain, exploratory designs are most productive. In the middle stages, when the researcher is quantifying patterns and testing relationships, descriptive designs take center stage. In the later stages, when the researcher is testing specific interventions and seeking to establish causal links, experimental designs become essential. Recognizing which stage the research program has reached helps the researcher select the design that is most appropriate for the current level of knowledge.

The fourth question involves resource constraints. Research designs vary enormously in their demands on time, money, and expertise. Exploratory designs and cross-sectional descriptive studies are generally the least resource-intensive, making them suitable when budgets are tight or timelines are short. Longitudinal descriptive designs require sustained investment over time, and experimental designs, particularly field experiments, can be the most expensive of all. A researcher who ignores resource constraints risks designing a study that cannot be completed, while a researcher who allows resources to dictate the design risks producing research that does not adequately address the question.

The fifth and final consideration is the stakes involved in the decision that the research will inform. When the stakes are high---when the decision is expensive, difficult to reverse, or has significant consequences for employees, customers, or the organization---the investment in causal research is justified, because the cost of acting on a wrong conclusion is substantial. When the stakes are lower, or when the decision is easily reversible, descriptive research may provide sufficient evidence to move forward. A company deciding whether to invest millions in a new product line should insist on causal evidence from controlled experiments. A company deciding which of two email subject lines to use for next week's newsletter may find a simple A/B test, or even a descriptive analysis of past open rates, perfectly adequate. The key is to match the rigor of the research to the magnitude and irreversibility of the decision it will inform.

---

## Recommended Resources

### Academic Texts
- Zikmund, W. G., Babin, B. J., Carr, J. C., & Griffin, M. (2013). *Business Research Methods* (9th ed.). Cengage Learning. --- Chapters 7--10.
- Churchill, G. A., & Iacobucci, D. (2015). *Marketing Research: Methodological Foundations*. Cengage Learning. --- Chapters 3--4.
- Campbell, D. T., & Stanley, J. C. (1963). *Experimental and Quasi-Experimental Designs for Research*. Houghton Mifflin. --- A classic reference on threats to validity.
- Cook, T. D., & Campbell, D. T. (1979). *Quasi-Experimentation: Design and Analysis Issues for Field Settings*. Houghton Mifflin.
- Creswell, J. W., & Creswell, J. D. (2018). *Research Design: Qualitative, Quantitative, and Mixed Methods Approaches* (5th ed.). SAGE Publications. --- Chapters 2--4.

### Online Resources
- Harvard Business Review: "A Refresher on A/B Testing" --- [hbr.org](https://hbr.org/2017/06/a-refresher-on-ab-testing)

### Companion Resource
- Patil, V. H. (2024). *Notes on Survey Design*, Chapter 1: The Research Process and the Role of Surveys --- [patilv.com/Survey-Design/chapters/chapter-1/](https://patilv.com/Survey-Design/chapters/chapter-1/)
