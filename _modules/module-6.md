---
title: "Survey Research and Questionnaire Design"
module_number: 6
author: "Vivek H. Patil, Ph.D."
institution: "Gonzaga University"
subtitle: "Designing surveys, constructing questionnaires, understanding measurement scales, and minimizing error."
learning_objectives:
  - "Identify the major sources of error in survey research and strategies to minimize them"
  - "Compare survey administration modes and their respective trade-offs"
  - "Define measurement and scaling, and distinguish among nominal, ordinal, interval, and ratio scales"
  - "Design Likert scales, semantic differential scales, and other attitude measurement instruments"
  - "Evaluate measurement accuracy through reliability and validity"
  - "Apply principles of questionnaire design, including question wording, order, and format"
  - "Conduct effective pretesting of survey instruments"
discussion_questions:
  - "Draft three survey questions about customer satisfaction with a restaurant—one that demonstrates good practice and two that illustrate common wording problems (double-barreled, leading, or vague). Explain what is wrong with the problematic questions and how to fix them."
  - "A researcher measures 'brand loyalty' by asking a single question: 'Are you loyal to Brand X? (Yes/No).' Critique this approach from the perspectives of measurement level, validity, and reliability. How would you improve the measurement?"
  - "The Gallup Poll and Harris Poll in 1969 asked about the same topic (troop withdrawal from Vietnam) but obtained very different results due to question framing. Find or create a modern example where question wording could produce similarly divergent results. What does this imply for survey design ethics?"
  - "Consider the concept of *uninformed response bias*—the tendency for respondents to offer opinions on topics they know nothing about. How does this challenge the assumption that survey data represent genuine attitudes? What safeguards can researchers implement?"
---

## 6.1 The Survey as a Research Method

The **survey** is the most widely used data collection method in business research. Surveys involve systematically asking a sample of individuals a set of questions and recording their responses. When designed well, surveys provide efficient, standardized data that can be analyzed statistically and generalized to broader populations.

However, surveys are also susceptible to numerous errors. A poorly designed survey can produce data that are not only useless but actively misleading. This module covers both the science of measurement and the art of questionnaire construction, equipping you to design surveys that produce valid, reliable, and actionable data.

## 6.2 Sources of Survey Error

Survey errors can be classified into several categories (adapted from Kumar, Aaker, & Day, 2002; Zikmund et al., 2013):

### Non-response Errors

Not everyone selected for a survey responds. **Non-response error** occurs when those who do not respond differ systematically from those who do. This is not merely a matter of having a smaller sample; the concern is that non-respondents may hold different attitudes, behaviors, or characteristics than respondents, introducing a bias that no amount of statistical adjustment can fully correct.

The reasons people decline to participate in surveys are varied and often interrelated. The perceived length or difficulty of the questionnaire is one of the most common deterrents: respondents who glance at a lengthy instrument and anticipate a burdensome time commitment may simply set it aside or click away. Lack of interest in the topic also suppresses participation, as people are naturally disinclined to invest time and cognitive effort in subjects they find irrelevant to their lives. Fear or suspicion about how the data will be used represents a growing concern in an era of data breaches and privacy scandals; respondents who worry that their answers might be sold, shared, or used against them may refuse to participate altogether or, if they do participate, provide guarded and inaccurate responses. Closely related is a general sense of invasion of privacy, particularly when surveys ask about income, health, relationships, or other topics that respondents consider personal. Finally, hostility toward the sponsor -- whether a corporation perceived as exploitative, a political organization with which the respondent disagrees, or even a university regarded with suspicion -- can suppress response rates and skew the resulting sample in ways that undermine the entire study.

### Factors Affecting Response Rates

Research has identified several factors that influence whether people choose to participate in surveys, and understanding these factors allows researchers to design data collection procedures that maximize cooperation.

Perceived workload is among the most powerful predictors: shorter questionnaires generally yield higher response rates than longer ones, and the relationship is not merely linear. A survey that takes five minutes to complete may achieve a dramatically higher response rate than one that takes twenty minutes, even if the additional fifteen minutes would have produced valuable data. The researcher must therefore weigh the marginal value of each additional question against its potential cost in reduced participation.

Topic interest exerts a strong influence as well. Respondents are substantially more likely to participate when the survey addresses a subject they care about, find relevant to their own experience, or believe has the potential to create positive change. This means that the same survey instrument may achieve very different response rates depending on the population to which it is administered, with implications for how the researcher frames the survey invitation and communicates the study's purpose and significance.

Sponsor credibility matters considerably. Surveys conducted by well-known, trusted organizations -- universities, government agencies, established research firms -- tend to achieve better response rates than those from unfamiliar or distrusted sources. The sponsor's name signals to the potential respondent whether the study is legitimate, whether their data will be treated responsibly, and whether the results are likely to be used for a worthwhile purpose.

Incentives, whether monetary (such as a cash payment, gift card, or lottery entry) or non-monetary (such as a summary of the results or a charitable donation made on the respondent's behalf), can meaningfully improve participation rates, though their effectiveness varies by context and population. Pre-notification -- contacting potential respondents before the survey arrives to alert them that it is coming and to explain its purpose -- has been shown to produce substantial improvements. Jolson (1977) found that telephone pre-notification before a mail survey increased response rates from 26.2% to 46.3% for a study on TV viewing habits, and from 20.5% to 68.2% for a collegiate clothing study. These findings illustrate that response rates are not fixed properties of a survey but are substantially influenced by the care and thoughtfulness the researcher invests in the data collection process.

### Response Inaccuracy

Even among those who do respond, the answers they provide may not be accurate, and the sources of inaccuracy are diverse and sometimes difficult to detect.

One broad category involves the **inability to respond** accurately. The respondent may simply not know the answer to the question being asked -- for example, a consumer asked to estimate how many times they visited a particular store in the past month may genuinely have no idea, yet feel compelled to provide a number rather than admit ignorance. Recall error is a pervasive problem: human memory is reconstructive rather than reproductive, and respondents routinely misremember the frequency, timing, and details of past behavior. They may telescope events (remembering something as more recent than it was), compress or expand time periods, or confuse one experience with another. These errors are not deliberate deception but rather inherent limitations of human cognition that no amount of good faith on the respondent's part can overcome.

The second broad category involves **unwillingness to respond accurately**, even when the respondent possesses the relevant information. Privacy concerns may lead respondents to underreport income, overreport charitable giving, or refuse to answer questions about sensitive topics. Social desirability bias -- the tendency to present oneself in a favorable light -- is one of the most pervasive and well-documented sources of response inaccuracy, leading people to overstate socially approved behaviors (exercise, voting, recycling) and understate socially disapproved ones (alcohol consumption, prejudice, unethical business practices). Prestige-seeking is a related phenomenon in which respondents exaggerate their education, job title, or material possessions. Courtesy bias, particularly common in face-to-face interviews, reflects a tendency to agree with the interviewer or to provide answers that the respondent believes the interviewer wants to hear. Acquiescence bias -- a general tendency to agree with statements regardless of their content -- can inflate scores on agree-disagree scales. Extremity bias, the tendency to select endpoints on rating scales regardless of true sentiment, can distort data in ways that are difficult to detect without careful scale analysis.

<div class="callout callout-case-study">
<p class="callout-title">Case Study: Uninformed Response Bias</p>
<p>Hawkins and Coney (1981) demonstrated a striking phenomenon: respondents often express opinions on topics they know nothing about. In a national survey, the general public gave the "National Bureau of Consumer Complaints" a positive effectiveness rating (mean = 2.72 on a 5-point scale)—even though 75% had never heard of it. (Indeed, the Bureau does not exist.) Among lawyers—who would be expected to know about such an organization—only 50% offered an opinion, and their ratings were less favorable (mean = 3.24). The lesson: <strong>the mere act of asking a question can create an opinion that did not previously exist</strong>.</p>
<p><em>Source: Hawkins, D. I. & Coney, K. A. (1981). Uninformed Response Error in Survey Research. Journal of Marketing Research, 18(August), 370–374.</em></p>
</div>

## 6.3 Survey Administration Modes

The choice of administration mode affects sampling, information quality, cost, speed, and response rates. Each mode has distinct advantages and limitations (adapted from Churchill & Iacobucci, 2002):

### Personal Interview (At Home or Mall Intercept)

Personal interviews, whether conducted in the respondent's home or through intercept approaches in public locations such as shopping malls, offer several compelling strengths. They typically achieve the highest response rates of any survey mode because the physical presence of a trained interviewer creates a social obligation to participate that is difficult to replicate through mail or electronic channels. The face-to-face format allows any type of question to be used, including those that require visual stimuli such as product samples, packaging mockups, or advertising materials. When respondents give incomplete or ambiguous answers to open-ended questions, the interviewer can probe for clarification and depth, yielding richer data than self-administered formats can produce. The interviewer can also control the sequencing of questions flexibly, adapting skip patterns and follow-up probes in real time based on the respondent's answers.

However, personal interviews carry significant limitations. Interviewer bias is a persistent concern: the interviewer's appearance, tone of voice, body language, and subtle verbal cues can all influence respondents' answers, and different interviewers may elicit systematically different responses from comparable respondents. Supervising interviewers who are dispersed across multiple locations is logistically challenging, making quality control more difficult than in centralized data collection modes. Personal interviews are generally the most expensive mode, particularly at-home interviews that require travel time, scheduling, and repeat visits to reach respondents who are not available on the first attempt. The at-home format is also among the slowest, since data collection depends on the pace at which individual interviews can be scheduled and conducted.

### Written Formats (Mail, Fax, Email, Web)

Written survey formats -- encompassing traditional mail questionnaires, fax-based instruments, email surveys, and web-based forms -- share a common defining characteristic: the respondent reads and answers the questions without the presence of an interviewer. This absence of an interviewer is simultaneously the greatest strength and the greatest weakness of written formats. On the positive side, interviewer bias is eliminated entirely: every respondent encounters exactly the same words, in exactly the same order, without any variation in tone, emphasis, or nonverbal cues. Respondents can work at their own pace, pausing to think about difficult questions, consulting records if needed, and returning to the questionnaire after interruptions. The anonymity afforded by self-administration encourages more honest responses to sensitive questions, particularly those involving income, health behaviors, or socially undesirable attitudes. Written formats are usually the least expensive mode on a per-respondent basis, and they can achieve wide geographic coverage without the travel costs associated with personal interviews.

The limitations of written formats are equally significant. Response rates tend to be lower than those achieved by personal interviews or telephone surveys, because there is no social obligation to respond to an envelope or email, and procrastination easily becomes permanent non-response. Ambiguous questions cannot be clarified: if a respondent misunderstands a question, there is no opportunity to redirect them, and the resulting data will reflect the misunderstanding rather than the respondent's true position. Open-ended questions typically yield shorter, less detailed responses in self-administered formats because there is no interviewer to probe for elaboration. The sequencing of questions is inflexible in paper-based formats (respondents can read ahead and answer questions out of order), and while web-based platforms offer sophisticated branching logic, the respondent's experience is still constrained by the predetermined design. Mail surveys are notoriously slow, with weeks or months often required to achieve a usable response rate, though email and web surveys have dramatically compressed this timeline.

### Telephone

Telephone surveys occupy a middle ground between personal interviews and written formats, combining some of the advantages of each while introducing their own distinctive strengths and challenges. Response rates are generally good -- better than mail but lower than in-person interviews -- because the real-time interaction with an interviewer creates a degree of social pressure to participate. Costs are moderate, since interviewers can work from a central location without travel expenses, and call-backs to reach unavailable respondents are far less costly than repeat home visits. Turnaround is quick: a well-staffed telephone survey can collect data from hundreds of respondents in a matter of days. Supervision is straightforward because interviewers work in a central facility where managers can monitor calls, provide real-time coaching, and ensure adherence to protocols. The telephone format also permits flexible question sequencing, with interviewers navigating skip patterns and adapting probes based on responses.

The limitations of telephone surveys have grown more pronounced in recent decades. No visual aids can be employed, eliminating the possibility of showing products, images, or complex response scales. Building rapport over the phone is more difficult than in person, and the absence of visual cues makes it harder for both interviewer and respondent to interpret nuance and emotion. Interviews must be kept relatively brief -- typically under 20 minutes -- because respondent patience and attention decline more rapidly over the phone than in face-to-face settings. Perhaps most importantly, representation problems have intensified as landline telephone ownership has declined, as more households rely exclusively on cell phones, and as caller ID and call-screening technology have made it easier for potential respondents to avoid unfamiliar callers.

### Online Surveys (Contemporary Standard)

In the years since these foundational taxonomies were developed, **online surveys** have become the dominant mode for many types of business research. Platforms such as Qualtrics, SurveyMonkey, Google Forms, and Microsoft Forms have made survey creation and deployment accessible to researchers at every level of technical sophistication.

The advantages of online surveys are substantial and help explain their rapid adoption. Costs are very low compared to every other mode, with many platforms offering free or inexpensive tiers for basic research needs. Deployment is rapid: a survey can move from final draft to live data collection in minutes, and responses accumulate in real time. Sophisticated logic and branching capabilities allow the survey to adapt dynamically to each respondent's answers, creating a personalized experience that mimics the flexibility of a trained interviewer. Multimedia elements -- images, audio clips, video segments -- can be embedded directly in the survey, enabling research designs that were previously possible only in person. Data are captured automatically in digital format, eliminating the transcription errors and data entry costs associated with paper-based methods. And the global reach of the internet means that a single survey can collect data from respondents across multiple countries, time zones, and languages.

The limitations of online surveys, however, are important and sometimes underappreciated. Sampling frame challenges represent a fundamental concern: when respondents are drawn from an online panel, the researcher must ask who is in that panel, how they were recruited, what incentives they receive, and whether they are representative of the population of interest. Self-selection bias is pervasive in online research, as participation is entirely voluntary and those who choose to respond may differ systematically from those who do not. Screen fatigue -- the weariness that comes from spending hours each day interacting with digital devices -- can depress response quality, leading to satisficing (choosing the first acceptable answer rather than the best answer), straight-lining (selecting the same response option for every item in a matrix), and premature abandonment. Device variability introduces a source of measurement error that researchers in previous eras did not face: the same survey may look and function quite differently on a desktop monitor, a tablet, and a smartphone, and these differences can affect how respondents interpret and answer questions. Finally, the digital divide means that populations with limited internet access -- including some elderly populations, rural communities, and lower-income groups -- may be systematically excluded from online research.

### Choosing a Survey Mode

Selecting the most appropriate survey mode requires careful consideration of several interrelated factors, and the decision should be driven by the specific requirements of the research question rather than by convenience or convention alone.

Sampling requirements are the logical starting point: the researcher must determine who needs to be reached and which modes can realistically access that population. A study of elderly consumers in rural communities may need a mail or telephone approach, while a study of digital marketing professionals might be best served by an online survey. Question complexity matters as well, because some research designs require visual aids, complex branching logic, or lengthy response formats that certain modes handle better than others. The sensitivity of the content should also guide mode selection, since respondents are typically more willing to disclose personal or socially undesirable information in anonymous, self-administered formats than in face-to-face or telephone interviews where another person is listening. Budget and timeline constraints inevitably play a role, as the ideal mode from a methodological standpoint may simply not be feasible given available resources and deadlines. Finally, the required response rate must be considered: if the research question demands a high response rate to minimize non-response bias, modes such as personal interviews or telephone surveys may be preferable to mail or online approaches, even if they cost more per completed interview.

## 6.4 Measurement and Scaling Fundamentals

**Measurement** is the standardized process of assigning numbers or other symbols to characteristics of objects of interest, according to pre-specified rules. **Scaling** is the process of creating a continuum on which objects are located according to the amount of a characteristic they possess (Churchill & Iacobucci, 2015).

The type of measurement depends on the type of data. Four levels of measurement form a hierarchy of increasing mathematical power:

### The Four Levels of Measurement

| Level | Properties | Permissible Operations | Central Tendency | Examples |
|---|---|---|---|---|
| **Nominal** | Identity only | =, ≠; Counting | Mode | Gender, brand names, jersey numbers, product categories |
| **Ordinal** | Identity + Order | =, ≠, >, <; Ranking | Mode, Median | Customer satisfaction rank, education level, competitive ranking |
| **Interval** | Identity + Order + Equal Distances | =, ≠, >, <, +, − | Mode, Median, Mean | Temperature (°F/°C), attitude scales, standardized test scores |
| **Ratio** | Identity + Order + Equal Distances + True Zero | =, ≠, >, <, +, −, ×, ÷ | Mode, Median, Arithmetic & Geometric Means | Age, income ($), sales volume, market share, weight |

These four levels form a cumulative hierarchy, meaning that each successive level possesses all the properties of the levels below it plus an additional defining property. Nominal measurement, the most basic level, provides only the ability to identify and classify -- to say that two objects are the same or different with respect to some characteristic. When we assign numerical codes to product categories (1 = electronics, 2 = apparel, 3 = groceries), those numbers serve purely as labels; arithmetic operations on them are meaningless. The mode (most frequently occurring category) is the only legitimate measure of central tendency.

Ordinal measurement adds the property of rank order, allowing the researcher to say not only that two objects differ but which one has more or less of the characteristic being measured. Customer satisfaction rankings, class standings, and competitive rankings are ordinal. We know that first place is better than second, but we do not know whether the gap between first and second is the same as the gap between second and third. Both the mode and the median are appropriate summary statistics, but the arithmetic mean is not, because computing a mean requires the assumption of equal intervals between adjacent scale points -- an assumption that ordinal data do not support.

Interval measurement adds the crucial property of equal distances between adjacent scale points, meaning that the difference between a score of 3 and a score of 4 is the same as the difference between a score of 7 and a score of 8. This property makes addition and subtraction meaningful and permits the use of the arithmetic mean, standard deviation, and the full range of parametric statistical procedures. Temperature scales are the classic example: the difference between 70 and 80 degrees Fahrenheit is the same as the difference between 30 and 40 degrees. However, interval scales lack a true zero point, so ratios are not meaningful -- 80 degrees is not "twice as hot" as 40 degrees.

Ratio measurement, the highest level, adds a true, non-arbitrary zero point that represents the complete absence of the characteristic being measured. Variables such as age, income in dollars, sales volume, and weight are ratio-scaled: zero means none, and ratios are meaningful. A company with $200 million in revenue genuinely has twice the revenue of a company with $100 million. This permits the full range of mathematical operations, including multiplication and division, and both arithmetic and geometric means are legitimate summary statistics.

### The Intrinsic vs. Measured Level

An important subtlety: a variable has an **intrinsic** measurement level (its true nature), but researchers can choose to measure it at a **lower** level. For example, income is intrinsically ratio-scaled, but surveys often measure it with ordinal categories (e.g., "Under $50,000 / $50,000–$100,000 / Over $100,000"). Measuring at a lower level **loses information** and limits the analyses that can be performed.

However, you **cannot** measure at a higher level than the intrinsic level. Gender is intrinsically nominal -- assigning numbers (Male = 1, Female = 0) does not make it interval or ratio, even though statistical software will happily compute a "mean" of such codes. The researcher must be vigilant about this.

## 6.5 Attitude Measurement Scales

Attitudes -- predispositions to respond favorably or unfavorably to objects, ideas, or people -- are central to much business research. Several scaling approaches are commonly used:

### Likert Scale

Respondents indicate their level of agreement with a series of statements, typically on a 5- or 7-point scale ranging from "Strongly Disagree" to "Strongly Agree."

*Example*: Rate your agreement with each statement:
1. The customer service representative was knowledgeable. (SD / D / N / A / SA)
2. The customer service representative was courteous. (SD / D / N / A / SA)
3. My issue was resolved promptly. (SD / D / N / A / SA)

### Semantic Differential Scale

Respondents rate an object on a series of bipolar adjective pairs (e.g., "Not Trustworthy — Trustworthy," "Unreliable — Reliable"), typically on a 7-point scale.

### Design Considerations for Single-Item Scales

Several design choices affect the quality of scale data, and each involves trade-offs that the researcher must evaluate in light of the study's objectives and the characteristics of the target respondent population.

| Design Choice | Considerations |
|---|---|
| **Number of categories** | More categories (5–7) provide finer discrimination but may confuse some respondents. Fewer categories (3–4) are simpler but less sensitive. |
| **Anchor strength** | "Colorful" vs. "Very colorful" vs. "Extremely colorful"—stronger anchors produce more moderate responses |
| **Category labeling** | All categories labeled (recommended) vs. only endpoints labeled |
| **Balance** | Balanced scales (equal positive and negative options) are generally superior to unbalanced scales |
| **Neutral midpoint** | Including a neutral option (e.g., "Neither Agree nor Disagree") avoids forcing respondents to choose a side but may attract satisficers |

The number of response categories is one of the most consequential design decisions. Scales with more categories -- five, seven, or even nine points -- provide finer discrimination, allowing respondents to express subtle gradations in their attitudes and enabling statistical analyses that benefit from greater variance. However, beyond a certain point, additional categories may overwhelm respondents who cannot meaningfully distinguish between, say, a "6" and a "7" on a nine-point scale. Scales with fewer categories are simpler and easier to use, particularly for populations with lower literacy or less experience with surveys, but they sacrifice sensitivity and may fail to detect differences that genuinely exist.

Anchor strength -- the intensity of the language used to label scale endpoints -- affects the distribution of responses in predictable ways. Stronger anchors such as "Extremely satisfied" tend to produce more moderate response distributions because respondents are reluctant to endorse extreme language, while milder anchors such as "Satisfied" make it easier for respondents to select the endpoint. Category labeling also matters: research consistently shows that scales in which every category is labeled with a descriptive word or phrase produce more reliable data than scales in which only the endpoints are labeled and intermediate positions are identified only by numbers. Balanced scales, which offer an equal number of positive and negative response options, are generally superior to unbalanced scales because they do not implicitly encourage responses in one direction. The inclusion of a neutral midpoint, such as "Neither Agree nor Disagree," remains a subject of methodological debate: it avoids forcing respondents who genuinely hold no opinion to choose a side, but it also provides a refuge for satisficers who would rather select a non-committal middle option than invest the cognitive effort required to evaluate the statement carefully.

<div class="callout callout-key-concept">
<p class="callout-title">Key Concept: The Impact of Scale Design on Responses</p>
<p>Schwarz et al. (1991) demonstrated that the numeric values assigned to response categories can change how respondents interpret them. When asked "How successful have you been in life?" on a 0-to-10 scale, respondents interpreted "0" as the absence of success. On a −5 to +5 scale, they interpreted "−5" as the presence of failure—a psychologically different anchor. The result: different response distributions from the same underlying population.</p>
<p><em>Source: Schwarz, N., Knäuper, B., Hippler, H.-J., Noelle-Neumann, E., & Clark, L. (1991). Rating Scales: Numeric Values May Change the Meaning of Scale Labels. Public Opinion Quarterly, 55(4), 570–582.</em></p>
</div>

## 6.6 The Effect of Category Ranges

Research by Rockwood, Sangster, and Dillman (1997) demonstrated that the range of response categories can dramatically influence responses. When asked about time spent studying:
- **Low categories** (Less than 0.5 / 0.5–1 / 1–1.5 / 1.5–2 / 2–2.5 / More than 2.5 hours): 23% reported studying 2.5+ hours
- **High categories** (Less than 2.5 / 2.5–3 / 3–3.5 / 3.5–4 / 4–4.5 / More than 4.5 hours): 69% reported studying 2.5+ hours

The categories served as an implicit frame of reference, signaling to respondents what was "normal." This finding has profound implications: **the design of the response scale can create the very data it purports to measure.**

## 6.7 Measurement Accuracy: Validity and Reliability

The quality of survey data depends fundamentally on the accuracy of the measurements. Accuracy has two core components:

**Observed Score = True Score + Systematic Error + Random Error**

### Validity
A measure is **valid** if it measures what it is supposed to measure. Differences in observed scores should reflect true differences in the characteristic being studied. There are several types of validity:
- **Face validity**: Does the measure *appear* to measure what it claims to? (Weakest form)
- **Content validity**: Does the measure cover the full domain of the concept?
- **Criterion validity**: Does the measure predict a known criterion? (Predictive and concurrent validity)
- **Construct validity**: Does the measure relate to other measures in theoretically expected ways? (Convergent and discriminant validity)

### Reliability
A measure is **reliable** if it produces **consistent results** across repeated administrations, different observers, or equivalent forms of the measure. Reliability is necessary but not sufficient for validity—a scale can be consistently wrong.

### Additional Accuracy Criteria
- **Sensitivity**: The extent to which the scale can discriminate among respondents who differ on the construct
- **Generalizability**: The ease with which the scale can be administered and interpreted across different settings and populations

## 6.8 Questionnaire Design Principles

Designing a questionnaire is both science and art. Even well-conceived measurement scales can fail if the questions are poorly worded, awkwardly ordered, or embedded in a confusing format.

### Question Wording Guidelines

**Use simple, direct, familiar vocabulary.** The language of a questionnaire should match the vocabulary and reading level of the target respondent population, not the vocabulary of the researcher or the sponsoring organization. Technical jargon, industry acronyms, and academic terminology that are second nature to the research team may be completely unfamiliar to respondents. A question that asks about "omnichannel retail integration" will confuse most consumers; a question that asks about "being able to shop seamlessly across the store, website, and app" communicates the same concept in accessible language. Complex sentence structures -- those with multiple embedded clauses, double negatives, or conditional phrasing -- increase the cognitive burden on respondents and raise the likelihood that different respondents will interpret the question differently. The goal is clarity, not elegance: every question should be interpretable in one and only one way by every respondent who reads it.

**Avoid vague or ambiguous words.** Words that seem perfectly clear to the questionnaire designer may carry very different meanings for different respondents. A classic example is the question "How often do you visit a fast-food restaurant?" paired with response options such as "Occasionally," "Sometimes," and "Often." These adverbs are subjective and unstable: "often" might mean once a week to one respondent and once a day to another. The solution is to replace vague frequency terms with concrete, behavioral anchors -- "Never," "1-2 times per month," "Once a week," "2-3 times per week," "Daily or more" -- that mean the same thing to everyone. Similarly, words like "usually," "most," "recently," and "a lot" should be replaced with specific quantities or time frames whenever possible.

**Avoid double-barreled questions.** A double-barreled question asks about two different things in a single question, making it impossible to know which part the respondent is answering. "Are you satisfied with the price and the service at Taco Bell?" is a classic example: a respondent who is satisfied with the service but dissatisfied with the price has no way to answer accurately. The fix is straightforward -- separate the question into two distinct items, one about price satisfaction and one about service satisfaction -- but double-barreled questions are surprisingly common in practice because they reflect the way people naturally talk about multifaceted experiences. The questionnaire designer must be vigilant, reviewing every question to ensure it asks about one and only one thing.

**Avoid leading or loaded questions.** A leading question contains language that pushes the respondent toward a particular answer. "Don't you think, because it's so greasy, fast food is one of the worst types of food?" is an extreme example, but subtler forms of leading are more common and more dangerous. Prefacing a question with "Most experts agree that..." or "Given the well-known problems with..." implicitly tells the respondent what the "correct" answer is. Loaded words -- those that carry strong positive or negative connotations -- can have the same effect. Asking whether a policy is "wasteful" or "efficient" is not the same as asking respondents to evaluate the policy's use of resources, even though the questionnaire designer might consider these equivalent. Neutral, balanced language is essential for eliciting genuine attitudes rather than manufactured agreement.

**Ensure questions are applicable to all respondents.** A question about "your spouse's income" is not applicable to single, divorced, or widowed respondents. A question about "your experience with our mobile app" is not applicable to customers who have never downloaded it. When questions are not universally applicable, respondents are forced either to skip items (creating missing data) or to fabricate answers (creating inaccurate data). The solution is to use filter questions or skip logic that route respondents past questions that do not apply to them. In online surveys, this can be handled seamlessly through programmed branching; in paper surveys, clear instructions ("If you answered 'No' to Question 12, please skip to Question 15") must be provided.

**Keep questions at an appropriate length.** Very long questions strain respondents' working memory: by the time they reach the end of a 50-word question, they may have forgotten the beginning or lost track of what they are being asked. Very short questions, on the other hand, may lack the context needed for respondents to answer meaningfully. A question like "Satisfaction?" provides no frame of reference -- satisfaction with what? over what time period? compared to what standard? The ideal question is long enough to provide necessary context and specificity but short enough to be grasped in a single reading. Pilot testing, discussed in Section 6.9, is the best way to determine whether questions achieve this balance for the target population.

### The Power of Question Framing

Classic polling data illustrate how framing affects responses:

**Gallup Poll (1969)**: "President Nixon has ordered the withdrawal of 25,000 US troops from Vietnam in the next three months. How do you feel about this—do you think troops should be withdrawn at a faster rate?"
- Faster: 42%, Same as now: 29%, Slower: 16%, No opinion: 13%

**Harris Poll (1969)**: "In general, do you feel the pace at which the president is withdrawing troops is too fast, too slow, or about right?"
- Too slow: 28%, About right: 49%, Too fast: 6%, No opinion: 18%

The same topic, asked with different frames, produced dramatically different results. This finding underscores the enormous responsibility that questionnaire designers bear.

### Response Category Design

The design of response categories requires the same care and precision as the wording of questions themselves, because poorly constructed response options can undermine even the most carefully worded question.

The principle of mutual exclusivity demands that each possible response fit into one and only one category. When categories overlap -- for example, income ranges of "$30,000-$50,000" and "$50,000-$70,000" that both include the $50,000 boundary -- a respondent whose true value falls at the overlap point has no clear way to answer, and the resulting ambiguity introduces measurement error. Every set of response categories should be reviewed to ensure that no response could logically fall into more than one option.

Exhaustiveness requires that the response categories collectively cover all possible answers. If a respondent's true answer is not represented among the available options, they are forced to choose the closest available alternative -- which may not be close at all -- or to skip the question entirely. Including an "Other (please specify)" option is a common safeguard, though researchers should monitor the frequency with which it is selected; if a large proportion of respondents choose "Other," the predefined categories may need revision.

Order effects -- the influence that the sequence of response options exerts on respondents' choices -- represent a subtler but well-documented concern. In lists of items, options presented first (primacy effect) or last (recency effect) may receive disproportionate selection, not because they are genuinely preferred but because of their position. Randomizing the order of response options across respondents, a feature available in most online survey platforms, is the most effective countermeasure.

Finally, the researcher must decide whether to include "Don't Know" or "No Opinion" options. Including them respects the reality that not every respondent has a formed opinion on every topic and prevents uninformed respondents from fabricating answers that pollute the data. However, omitting these options forces respondents to engage with the question and may surface latent attitudes that would otherwise go unexpressed. The decision depends on the research context: for topics where uninformed responses are a significant risk, including a "Don't Know" option is generally advisable; for topics where nearly all respondents can be expected to have opinions, the option may be unnecessary and could simply attract satisficers seeking an easy way out.

## 6.9 Pretesting

Before fielding a survey, **pretesting** is essential. Pretesting involves administering the questionnaire to a small sample under conditions similar to the actual study, with the goal of identifying problems before they contaminate the full data collection.

Pretesting is not a formality or a box to be checked; it is a substantive phase of the research process that can reveal problems invisible to even the most experienced questionnaire designer. The value of pretesting lies in its ability to expose the gap between how the researcher intended questions to be understood and how actual respondents understand them -- a gap that is often wider and more consequential than the researcher expects.

Effective pretesting evaluates several dimensions of questionnaire performance simultaneously. It examines whether responses show adequate **variation**: if nearly all pretest respondents give the same answer to a question, the question may not be discriminating among people who genuinely differ on the underlying construct, and it may need to be revised to increase sensitivity. Pretesting evaluates **meaning** by asking respondents not just to answer questions but to paraphrase them, explaining in their own words what they believe each question is asking. This technique, sometimes called cognitive interviewing, frequently reveals misinterpretations that would go undetected if the researcher examined only the numerical responses.

**Task difficulty** is assessed by observing whether respondents can answer questions without visible confusion, excessive hesitation, or requests for clarification. Questions that consistently produce furrowed brows, long pauses, or requests for help are signaling that something in the wording, structure, or response format needs to be simplified. **Respondent interest and attention** are evaluated by monitoring engagement throughout the pretesting session: if respondents begin to rush through questions, provide increasingly superficial answers, or express frustration or boredom, the questionnaire may be too long, too repetitive, or insufficiently engaging in its later sections.

The **flow** of the questionnaire -- the logical progression from topic to topic and from question to question -- is tested by asking respondents whether the sequence felt natural or whether certain transitions were jarring or confusing. **Skip patterns** and branching logic must be verified to ensure that respondents are routed correctly through the instrument and that no one encounters a question that is inapplicable to their situation. Finally, the overall **length** of the questionnaire is assessed by timing the pretest administration and comparing it to the researcher's expectations and to the tolerance of the target population. A questionnaire that consistently takes 25 minutes to complete when the researcher expected 15 minutes is a questionnaire that needs to be shortened.

The results of pretesting should be taken seriously and acted upon, even when the required revisions are inconvenient or time-consuming. The cost of pretesting and revising is trivial compared to the cost of fielding a flawed instrument to the full sample and discovering the problems only when it is too late to fix them.

<div class="callout callout-jesuit">
<p class="callout-title">Jesuit Perspective</p>
<p>The meticulous attention to questionnaire design—ensuring questions are clear, fair, unbiased, and respectful of respondents' time and intelligence—is an expression of <em>cura personalis</em> in research practice. Every poorly worded, leading, or confusing question represents a failure to respect the participant. Every well-crafted question represents an act of care. The Jesuit insistence on doing things well—<em>magis</em>—demands that we invest the effort to get measurement right.</p>
</div>

---

## Recommended Resources

### Academic Texts
- Churchill, G. A., & Iacobucci, D. (2015). *Marketing Research: Methodological Foundations*. Cengage Learning. — Chapters 9–12.
- Dillman, D. A., Smyth, J. D., & Christian, L. M. (2014). *Internet, Phone, Mail, and Mixed-Mode Surveys: The Tailored Design Method* (4th ed.). Wiley.
- DeVellis, R. F., & Thorpe, C. T. (2022). *Scale Development: Theory and Applications* (5th ed.). SAGE Publications.

### Key Articles
- Hawkins, D. I. & Coney, K. A. (1981). Uninformed Response Error in Survey Research. *Journal of Marketing Research*, 18(August), 370–374.
- Schwarz, N., et al. (1991). Rating Scales: Numeric Values May Change the Meaning of Scale Labels. *Public Opinion Quarterly*, 55(4), 570–582.
- Rockwood, T. H., Sangster, R. L., & Dillman, D. A. (1997). The Effect of Response Categories on Questionnaire Answers. *Sociological Methods & Research*, 26(1), 118–140.
- Jolson, M. A. (1977). How to Double or Triple Mail-Survey Response Rates. *Journal of Marketing*, 41(4), 78–81.

### Companion Resource
- Patil, V. H. (2024). *Notes on Survey Design*, Chapter 2: Survey Process and Errors; Chapter 3: Measurement and Scaling — [patilv.com/Survey-Design/](https://patilv.com/Survey-Design/)
